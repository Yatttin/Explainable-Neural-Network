{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Movie_review_Anchor&Afinn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZysD1e4vHmNw",
        "outputId": "163dab5a-f8b4-43ae-8b1f-906b7311d40f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install anchor_exp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting anchor_exp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/0a/e8bde54744085a9d3ddbb4a4a8531386b24dcab0c61d6eec7bdec2032194/anchor_exp-0.0.1.0.tar.gz (428kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from anchor_exp) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from anchor_exp) (1.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from anchor_exp) (2.2.4)\n",
            "Collecting lime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/ee/4aaac4cd79f16329746495aca96f8c35f278b5c774eff3358eaa21e1cbf3/lime-0.2.0.0.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.6/dist-packages (from anchor_exp) (0.22.2.post1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (0.6.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (46.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (0.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from lime->anchor_exp) (3.2.1)\n",
            "Collecting pillow==5.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/5e/e91792f198bbc5a0d7d3055ad552bc4062942d27eaf75c3e2783cf64eae5/Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.6/dist-packages (from lime->anchor_exp) (0.16.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.22->anchor_exp) (0.15.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->anchor_exp) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->anchor_exp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->anchor_exp) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->anchor_exp) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->anchor_exp) (1.6.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime->anchor_exp) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime->anchor_exp) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime->anchor_exp) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime->anchor_exp) (0.10.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime->anchor_exp) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime->anchor_exp) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime->anchor_exp) (2.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->anchor_exp) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->lime->anchor_exp) (1.12.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime->anchor_exp) (4.4.2)\n",
            "Building wheels for collected packages: anchor-exp, lime\n",
            "  Building wheel for anchor-exp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for anchor-exp: filename=anchor_exp-0.0.1.0-cp36-none-any.whl size=434742 sha256=5f6978a7be0f4a440bf9e7274680a36e31d7b7deadf5843ad021a0a4f68ab02f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/b7/cd/7bfb36f4a01ff6b1509cd3432f8b208f455d232cee1079e309\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.0-cp36-none-any.whl size=284181 sha256=1bcf1bc5886cd37fd170676b5a7526aa8c99fe05702aa39cb28313fb5508f528\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f2/ec/e5ebd07348b2b1ac722e91c2f549fcc220f7d5f25497a61232\n",
            "Successfully built anchor-exp lime\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pillow, lime, anchor-exp\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed anchor-exp-0.0.1.0 lime-0.2.0.0 pillow-5.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tErZayUxHmNz",
        "outputId": "ced1cb95-8a03-45d6-c422-5d7ecde92fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        }
      },
      "source": [
        "pip install spacy && python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (46.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=451ae39b2879e34fff08c787eccab0ca264c5fad5e687908970f7610395b3c97\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q7_mqn_5/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTmYZI0JOlPS"
      },
      "source": [
        "import time\n",
        "import en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGZC8ZxUHmN3"
      },
      "source": [
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "import sklearn.linear_model\n",
        "import sklearn.ensemble\n",
        "import spacy\n",
        "import sys\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from anchor import anchor_text\n",
        "import time\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi8l8EEsHmN7"
      },
      "source": [
        "def load_polarity(path):\n",
        "    data = []\n",
        "    labels = []\n",
        "    f_names = ['d.neg','d.pos']\n",
        "    for (a, f) in enumerate(f_names):\n",
        "        print(a,f)\n",
        "        for line in open(f, 'rb'):\n",
        "            try:\n",
        "                line.decode('utf8')\n",
        "            except:\n",
        "                continue\n",
        "            data.append(line.strip())\n",
        "            labels.append(a)\n",
        "\n",
        "\n",
        "    return data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQypHtHlHmN9",
        "outputId": "5f564d38-a201-475b-8d68-72db1e996406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "data, labels = load_polarity('')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 d.neg\n",
            "1 d.pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOU1KUMHiPWm",
        "outputId": "b689b6d3-65b1-44de-bcd5-51d6666b95a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "print(len(data))\n",
        "print(len(labels))\n",
        "print(data[8])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18958\n",
            "18958\n",
            "b'\"Tokyo Eyes\" tells of a 17 year old Japanese girl who falls in like with a man being hunted by her big bro who is a cop. This lame flick is about 50% filler and 50% talk, talk, and more talk. You\\'ll get to see the less than stellar cast of three as they talk on the bus, talk and play video games, talk and get a haircut, talk and walk and walk and talk, talk on cell phones, hang out and talk, etc. as you read subtitles waiting for something to happen. The thin wisp of a story is not sufficient to support a film with low end production value, a meager cast, and no action, no romance, no sex or nudity, no heavy drama...just incessant yadayadayada\\'ing. (C-)'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlAhOuAFoJo4"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH6cmeTBFHpY",
        "outputId": "6b7aebde-1074-4f4a-8d8c-65185d76c593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "random_state = None\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import re, string, unicodedata\n",
        "import inflect\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import stopwords as sw\n",
        "sww = sw.words()\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sklearn\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD5SXrdEOryE"
      },
      "source": [
        "def clean(text):\n",
        "    clean_data = []\n",
        "    for corpus in text:\n",
        "        #print(corpus)\n",
        "        if type(corpus) is bytes:\n",
        "            corpus = corpus.decode('utf-8')\n",
        "        #chnage word to lower case, we want to ignore capital or lower cases\n",
        "        lower_doc = corpus.lower()\n",
        " \n",
        "        #expan construction of words into it's full representation\n",
        "        #expand_doc = expandContractions(lower_doc)\n",
        "        \n",
        "        #delete punctuation\n",
        "        clean_doc = re.sub(r'[^\\w\\s]','',lower_doc)\n",
        "        \n",
        "        #split sentence into words\n",
        "        tokenized_doc1 = word_tokenize(clean_doc)\n",
        "        \n",
        "        sww = sw.words() \n",
        "        new_corpus = [w for w in tokenized_doc1 if not w in sww]\n",
        "        clean_data.append(new_corpus)\n",
        "    return clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqG9lGUlD3GN"
      },
      "source": [
        "data_new = clean(data)\n",
        "del data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3HsFbnMJdjZ",
        "outputId": "d8d00f17-dd7f-47dc-de96-5e352beb239f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(data_new))\n",
        "print(len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18958\n",
            "18958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFbWCuDyHmOE"
      },
      "source": [
        "train, test, train_labels, test_labels = sklearn.model_selection.train_test_split(data_new, labels, test_size=.2, random_state=42)\n",
        "train, val, train_labels, val_labels = sklearn.model_selection.train_test_split(train, train_labels, test_size=.1, random_state=42)\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "val_labels = np.array(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A25kKMdOHt-",
        "outputId": "66e25ea1-3896-481f-9187-a3cc66ed9fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['true', 'story', 'great', 'pianist', 'jazz', 'singerlegend', 'ray', 'charles', 'oscar', 'bafta', 'golden', 'globe', 'winning', 'jamie', 'foxx', 'born', 'poor', 'african', 'americantown', 'went', 'blind', '7', 'years', 'old', 'skills', 'touch', 'hearing', 'would', 'later', 'life', 'would', 'lead', 'stardom', '1960s', 'accomplished', 'dream', 'selling', 'records', 'millions', 'leading', 'charts', 'songs', 'albums', 'story', 'showed', 'downfalls', 'including', 'separation', 'wife', 'child', 'affair', 'band', 'member', 'alcohol', 'use', 'going', 'prison', 'starring', 'regina', 'king', 'margie', 'hendricks', 'kerry', 'washington', 'bea', 'robinson', 'clifton', 'powell', 'jeff', 'brown', 'harry', 'lennix', 'joe', 'adams', 'bokeem', 'woodbine', 'fathead', 'newman', 'aunjanue', 'ellis', 'mary', 'ann', 'fisher', 'sharon', 'warren', 'aretha', 'robinson', 'cj', 'sanders', 'young', 'ray', 'robinson', 'curtis', 'armstrong', 'ahmet', 'ertegun', 'richard', 'schiff', 'jerry', 'wexler', 'great', 'story', 'great', 'singer', 'impression', 'songs', 'including', 'hit', 'road', 'jack', 'highlights', 'oscar', 'best', 'sound', 'mixing', 'nominated', 'best', 'costume', 'design', 'best', 'director', 'taylor', 'hackford', 'best', 'editing', 'best', 'motion', 'picture', 'year', 'bafta', 'best', 'sound', 'nominated', 'anthony', 'asquith', 'award', 'film', 'music', 'craig', 'armstrong', 'best', 'original', 'screenplay', 'nominated', 'golden', 'globe', 'best', 'motion', 'picture', 'musical', 'comedy', 'number', '99', '100', 'years', '100', 'cheers', 'good']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnw4Il9gHmOI"
      },
      "source": [
        "vectorizer = CountVectorizer(min_df=1)\n",
        "\n",
        "def to_str(text):\n",
        "  text_str = []\n",
        "  for line in text:\n",
        "     text_str.append(\" \".join(line))\n",
        "  return text_str\n",
        "\n",
        "\n",
        "train_str = to_str(train)\n",
        "test_str = to_str(test)\n",
        "val_str = to_str(val)\n",
        "vectorizer.fit(train_str)\n",
        "\n",
        "train_vectors = vectorizer.transform(train_str)\n",
        "test_vectors = vectorizer.transform(test_str)\n",
        "val_vectors = vectorizer.transform(val_str)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ2ZZOSCLyNI",
        "outputId": "da993854-4a05-49f9-df4a-ad4a3d2730f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(train_vectors.shape)\n",
        "print(test_vectors.shape)\n",
        "print(val_vectors.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13649, 84540)\n",
            "(3792, 84540)\n",
            "(1517, 84540)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec8WIXjvHmOP"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmX10o5FHmOT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch import Tensor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV-Fj0DlHmOX"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        num_input = 84540\n",
        "        num_classes = 2\n",
        "        hidden_size1 = 256\n",
        "        hidden_size2 = 256\n",
        "        self.linear1 = nn.Linear(num_input, hidden_size1)\n",
        "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.linear3 = nn.Linear(hidden_size2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden1 = self.linear1(x)\n",
        "        nn.Dropout(0.4)\n",
        "        activation1 = F.relu(hidden1)\n",
        "        hidden2 = self.linear2(activation1)\n",
        "        nn.Dropout(0.4)\n",
        "        activation2 = F.relu(hidden2)\n",
        "        out = self.linear2(activation2)\n",
        "        ou t = F.log_softmax(out, dim=1)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9DnpnOHHmOb"
      },
      "source": [
        "def prepare_batch(data, labels, size):\n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(data.shape[0]), size, replace=True)\n",
        "\n",
        "    for i in random_index:\n",
        "        random_inputs.append(data[i])\n",
        "        random_labels.append(labels[i])\n",
        "    return np.array(random_inputs), np.array(random_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBkmwHGMHmOf",
        "outputId": "5dab1118-89bc-4b23-c0b7-bfd6870f217b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "#get the model\n",
        "net = Net().to(device)\n",
        "\n",
        "#learning rate\n",
        "learning_rate=0.1\n",
        "\n",
        "#calculate loss by function. The negative log likelihood loss.\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Define the optimiser\n",
        "# Pass the model parameters to be updated, and setup the learning rate when calling optim.SGD\n",
        "# Please find the detailed information about the SGD optimiser in PyTorch (https://pytorch.org/docs/stable/optim.html).\n",
        "optimiser = optim.SGD(net.parameters(), lr=learning_rate) \n",
        "\n",
        "#epochs\n",
        "no_of_epochs = 200\n",
        "\n",
        "# Every epoch, the model will be modified by the given learning rate\n",
        "for epoch in range(no_of_epochs):  \n",
        "\n",
        "    # get the input and label\n",
        "    inputs, labels = prepare_batch(train_vectors.toarray(), train_labels, 300)\n",
        "    inputs = torch.from_numpy(inputs).float().to(device)\n",
        "    labels = torch.from_numpy(labels).long().to(device)\n",
        "    \n",
        "    net.train()\n",
        "\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimiser.step() # back propagation\n",
        "\n",
        "    if epoch % 20 == 0:    # print every 100 epochs\n",
        "        print('%d, loss: %.3f' %(epoch + 1, loss.item()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1, loss: 5.588\n",
            "21, loss: 1.402\n",
            "41, loss: 0.841\n",
            "61, loss: 0.876\n",
            "81, loss: 0.576\n",
            "101, loss: 0.578\n",
            "121, loss: 0.539\n",
            "141, loss: 0.422\n",
            "161, loss: 0.479\n",
            "181, loss: 0.333\n",
            "201, loss: 0.230\n",
            "221, loss: 0.491\n",
            "241, loss: 0.191\n",
            "261, loss: 0.457\n",
            "281, loss: 0.174\n",
            "301, loss: 0.224\n",
            "321, loss: 0.145\n",
            "341, loss: 0.361\n",
            "361, loss: 0.192\n",
            "381, loss: 0.145\n",
            "401, loss: 0.150\n",
            "421, loss: 0.124\n",
            "441, loss: 0.521\n",
            "461, loss: 0.273\n",
            "481, loss: 0.123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6Nj1XeFHmOl",
        "outputId": "e849e2ea-c56d-43e7-f32e-09e1d918e619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "test_torch = torch.from_numpy(test_vectors.toarray()).float().to(device)\n",
        "\n",
        "output = net(test_torch)\n",
        "_, predicted = torch.max(output, 1)\n",
        "predicted = Tensor.numpy(predicted)\n",
        "print(\"Accuracy: %s\" % np.mean(predicted == test_labels))\n",
        "print(classification_report(test_labels, predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8850210970464135\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.88      0.89      2072\n",
            "           1       0.86      0.89      0.88      1720\n",
            "\n",
            "    accuracy                           0.89      3792\n",
            "   macro avg       0.88      0.89      0.88      3792\n",
            "weighted avg       0.89      0.89      0.89      3792\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXipUhEUHmOr"
      },
      "source": [
        "## Anchor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpuy-OTcf2kn"
      },
      "source": [
        "nlp = en_core_web_lg.load()\n",
        "#test_str = to_str(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzZZo-qgHmOs"
      },
      "source": [
        "def predict_nn(texts):\n",
        "    vec = vectorizer.transform(texts)\n",
        "    x_torch = torch.from_numpy(vec.toarray()).float().to(device)\n",
        "    output = net(x_torch)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    predicted = Tensor.numpy(predicted).tolist()\n",
        "    return np.array(predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN6b1b6g-b26",
        "outputId": "90e6613d-c89c-41f2-cc19-7c8ade7a29d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "predict_nn([text])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h3Iqg6iHmOz",
        "outputId": "c83e5669-e59d-4298-f6ef-b231293dc3ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "idx = 0\n",
        "\n",
        "explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=True)\n",
        "np.random.seed(1)\n",
        "text = test_str[idx]\n",
        "pred = explainer.class_names[predict_nn([text])[idx]]\n",
        "alternative =  explainer.class_names[1 - predict_nn([text])[0]]\n",
        "print('Prediction: %s' % pred)\n",
        "print('True label: %s' % 'positive' if test_labels[idx] == 1 else 'True label: %s' % 'negative')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: positive\n",
            "True label: positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtKS4SdYibKO",
        "outputId": "3fc0f11e-96ac-4b04-f975-6197578e0162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "466"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuYkIQRLiUz4"
      },
      "source": [
        "exp = explainer.explain_instance(text, predict_nn, threshold=0.90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uflRP-mGHmO8",
        "outputId": "528c9f46-3c11-4328-8008-8787f12a40bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
        "print('Precision: %.2f' % exp.precision())\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % pred)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(only_same_prediction=True)]))\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(partial_index=0, only_different_prediction=True)]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anchor: situation AND bugs\n",
            "Precision: 0.94\n",
            "\n",
            "Examples where anchor applies and model predicts positive:\n",
            "\n",
            "UNK sylvester cartoons made UNK pair UNK bugs bunny terms UNK tunes UNK UNK besi\n",
            "UNK UNK cartoons made legendary UNK UNK bugs bunny terms looney tunes popularity\n",
            "tweety UNK UNK UNK UNK pair UNK bugs UNK UNK UNK tunes UNK UNK UNK UNK UNK UNK s\n",
            "tweety sylvester UNK UNK legendary pair UNK bugs UNK terms UNK tunes UNK br UNK \n",
            "UNK UNK cartoons UNK UNK UNK UNK bugs UNK UNK looney tunes UNK UNK UNK stock UNK\n",
            "tweety sylvester UNK UNK legendary UNK second bugs UNK UNK UNK tunes UNK UNK UNK\n",
            "UNK UNK UNK UNK legendary pair UNK bugs bunny UNK looney tunes popularitybr UNK \n",
            "UNK UNK UNK made UNK pair second bugs bunny terms UNK UNK popularitybr UNK UNK U\n",
            "tweety UNK cartoons UNK legendary pair UNK bugs UNK UNK looney UNK UNK UNK besid\n",
            "tweety sylvester UNK UNK legendary pair UNK bugs bunny terms UNK UNK UNK br besi\n",
            "\n",
            "Examples where anchor applies and model predicts negative:\n",
            "\n",
            "UNK UNK UNK made UNK UNK UNK UNK bunny UNK looney UNK UNK UNK besides UNK situat\n",
            "UNK UNK UNK made UNK UNK second bugs bunny terms looney tunes popularitybr UNK b\n",
            "UNK sylvester UNK made legendary pair second bugs UNK UNK UNK tunes popularitybr\n",
            "tweety sylvester UNK UNK UNK UNK second UNK UNK UNK UNK UNK UNK br UNK stock sit\n",
            "UNK UNK cartoons made UNK pair UNK bugs bunny terms UNK UNK UNK br UNK stock UNK\n",
            "UNK UNK UNK made legendary pair UNK bugs UNK terms UNK UNK popularitybr br UNK U\n",
            "tweety sylvester UNK made UNK pair UNK UNK UNK UNK UNK tunes UNK br UNK UNK situ\n",
            "tweety UNK UNK made UNK pair second UNK UNK UNK UNK UNK popularitybr UNK UNK sto\n",
            "UNK UNK UNK made legendary UNK second UNK bunny UNK UNK tunes popularitybr UNK b\n",
            "UNK UNK cartoons made UNK UNK UNK bugs bunny UNK UNK UNK popularitybr UNK UNK st\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCOMtHTiQnME",
        "outputId": "fd792340-aaf7-431a-b347-9b3c9d6c75c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "source": [
        "# Sample text is too long to vias\n",
        "exp.show_in_notebook()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-5f099f953b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sample text is too long to vias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_in_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/anchor/anchor_explanation.py\u001b[0m in \u001b[0;36mshow_in_notebook\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_in_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/anchor/anchor_explanation.py\u001b[0m in \u001b[0;36mas_html\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_html_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_in_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/anchor/anchor_text.py\u001b[0m in \u001b[0;36mas_html\u001b[0;34m(self, exp)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mexample_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'examples'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0mexample_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         explanation = {'names': exp['names'],\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/anchor/anchor_text.py\u001b[0m in \u001b[0;36mprocess_examples\u001b[0;34m(examples, idx)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                     \u001b[0mraw_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rawIndexes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mraw_indexes\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mout_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/anchor/anchor_text.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                     \u001b[0mraw_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rawIndexes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mraw_indexes\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mout_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.__getitem__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.bounds_check\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: [E026] Error accessing token at position 49: out of bounds in Doc of length 13."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txlQbTLCHmPB"
      },
      "source": [
        "## LIME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wqBdiTmHmPC"
      },
      "source": [
        "from lime import lime_text\n",
        "from lime.lime_text import LimeTextExplainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFf1oRu2PoDI"
      },
      "source": [
        "#pip install lime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ojfHrT3HmPI"
      },
      "source": [
        " def findprob(test):\n",
        "\n",
        "    test = vectorizer.transform(test)\n",
        "\n",
        "    test = torch.from_numpy(test.toarray()).float().to(device)\n",
        "    pred_outputs = net(test)\n",
        "    out = pred_outputs.data.cpu().numpy()\n",
        "    exp = np.exp(out)\n",
        "    expsum = 0\n",
        "    for i in range(len(exp[0])):\n",
        "        expsum += exp[0][i]\n",
        "\n",
        "    \n",
        "    #prob = prob.astype('float')\n",
        "    return exp/expsum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVb8vkfTHmPP"
      },
      "source": [
        "class_names = ['neg', \"pos\"]\n",
        "explainer_L = LimeTextExplainer(class_names=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gA44K3KHmPW",
        "outputId": "b2d81ac8-0d12-4a50-8ba3-cd10db3ffe72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "idx = 0\n",
        "print('Document id: %d' % idx)\n",
        "pred_outputs = net(test_torch[idx].reshape(-1, 84540))\n",
        "predicted = torch.argmax(pred_outputs, 1)\n",
        "int(predicted[0].data.cpu().numpy())\n",
        "print('Predicted class =', class_names[int(predicted[0].data.cpu().numpy())])\n",
        "print('True class: %s' % class_names[test_labels[idx]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document id: 0\n",
            "Predicted class = pos\n",
            "True class: pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vf99iNbHmPm",
        "outputId": "861b4695-885c-43c4-92cf-2c55b634f5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "exp_L = explainer_L.explain_instance(test_str[idx], findprob, num_features=5, top_labels = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:114: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYaVUtbHnzER",
        "outputId": "170bec53-b76f-4611-bd6f-ed8ff16beade",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "exp_L.as_list()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sylvester', 0.1514226384370585),\n",
              " ('situation', 0.09157544000410336),\n",
              " ('pair', -0.08451584269305518),\n",
              " ('time', 0.07924365966418842),\n",
              " ('garbage', -0.07000775502614492)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ADlldZcuHmPr"
      },
      "source": [
        "#exp.show_in_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cET7kSE_vsa"
      },
      "source": [
        "# Afinn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1KPOK62eQ1D",
        "outputId": "886d6cbd-d193-42eb-fe40-ecb5ce01ca1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "pip install afinn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting afinn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/e5/ffbb7ee3cca21ac6d310ac01944fb163c20030b45bda25421d725d8a859a/afinn-0.1.tar.gz (52kB)\n",
            "\r\u001b[K     |██████▎                         | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-cp36-none-any.whl size=53452 sha256=7e4bf65eebba249f43f3825e42fbad2b62e14dc143d9e1af94ff6f02f76946f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/1c/de/428301f3333ca509dcf20ff358690eb23a1388fbcbbde008b2\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3SH2FqlHmP1",
        "outputId": "7caf2fcc-7bbc-4bd1-a03d-9f7f9d9891b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from afinn import Afinn\n",
        "from nltk.corpus import gutenberg\n",
        "import textwrap\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRC47t4Eoj-B"
      },
      "source": [
        "afinn = Afinn()\n",
        "def afi(text, target, topk):\n",
        "    dic = {}\n",
        "    for w in text:  \n",
        "        dic[w] = afinn.score(w)\n",
        "    if target == 0:\n",
        "        A = sorted(dic.items(), key = lambda x:x[1])\n",
        "        return [A[i][0] for i in range(topk)]\n",
        "    if target == 1:\n",
        "        A = sorted(dic.items(), key = lambda x:x[1], reverse = True)\n",
        "        print(A)\n",
        "        return [A[i][0] for i in range(topk)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhVRkDmTgni6",
        "outputId": "4bd5d106-403d-4af7-8d76-75b5040dadb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "test_str[idx]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tweety sylvester cartoons made legendary pair second bugs bunny terms looney tunes popularitybr br besides stock situations duo sylvester feeding garbage cans tawt taw putty tat line etc tweetys sos stars granny types supporting characters warner bros classics abundance enrich color flavor time bird aboard ocean liner gags extracted situation creative livelybr br day cartoon history friz freleng decided pair sylvester departed robert clampetts little yellow bird'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Ba387_GSVv",
        "outputId": "b0efd06e-3b79-4a92-ed79-1c3a7b871d26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "idx = 0\n",
        "te = word_tokenize(test_str[idx])\n",
        "afi(te, test_labels[idx], 5)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('creative', 2.0), ('supporting', 1.0), ('aboard', 1.0), ('tweety', 0.0), ('sylvester', 0.0), ('cartoons', 0.0), ('made', 0.0), ('legendary', 0.0), ('pair', 0.0), ('second', 0.0), ('bugs', 0.0), ('bunny', 0.0), ('terms', 0.0), ('looney', 0.0), ('tunes', 0.0), ('popularitybr', 0.0), ('br', 0.0), ('besides', 0.0), ('stock', 0.0), ('situations', 0.0), ('duo', 0.0), ('feeding', 0.0), ('cans', 0.0), ('tawt', 0.0), ('taw', 0.0), ('putty', 0.0), ('tat', 0.0), ('line', 0.0), ('etc', 0.0), ('tweetys', 0.0), ('sos', 0.0), ('stars', 0.0), ('granny', 0.0), ('types', 0.0), ('characters', 0.0), ('warner', 0.0), ('bros', 0.0), ('classics', 0.0), ('abundance', 0.0), ('enrich', 0.0), ('color', 0.0), ('flavor', 0.0), ('time', 0.0), ('bird', 0.0), ('ocean', 0.0), ('liner', 0.0), ('gags', 0.0), ('extracted', 0.0), ('situation', 0.0), ('livelybr', 0.0), ('day', 0.0), ('cartoon', 0.0), ('history', 0.0), ('friz', 0.0), ('freleng', 0.0), ('decided', 0.0), ('departed', 0.0), ('robert', 0.0), ('clampetts', 0.0), ('little', 0.0), ('yellow', 0.0), ('garbage', -1.0)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['creative', 'supporting', 'aboard', 'tweety', 'sylvester']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9nGpGKVp4Z2",
        "outputId": "54330918-7dc9-41b8-b47a-b5c18fb6cae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " test[idx]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tweety',\n",
              " 'sylvester',\n",
              " 'cartoons',\n",
              " 'made',\n",
              " 'legendary',\n",
              " 'pair',\n",
              " 'second',\n",
              " 'bugs',\n",
              " 'bunny',\n",
              " 'terms',\n",
              " 'looney',\n",
              " 'tunes',\n",
              " 'popularitybr',\n",
              " 'br',\n",
              " 'besides',\n",
              " 'stock',\n",
              " 'situations',\n",
              " 'duo',\n",
              " 'sylvester',\n",
              " 'feeding',\n",
              " 'garbage',\n",
              " 'cans',\n",
              " 'tawt',\n",
              " 'taw',\n",
              " 'putty',\n",
              " 'tat',\n",
              " 'line',\n",
              " 'etc',\n",
              " 'tweetys',\n",
              " 'sos',\n",
              " 'stars',\n",
              " 'granny',\n",
              " 'types',\n",
              " 'supporting',\n",
              " 'characters',\n",
              " 'warner',\n",
              " 'bros',\n",
              " 'classics',\n",
              " 'abundance',\n",
              " 'enrich',\n",
              " 'color',\n",
              " 'flavor',\n",
              " 'time',\n",
              " 'bird',\n",
              " 'aboard',\n",
              " 'ocean',\n",
              " 'liner',\n",
              " 'gags',\n",
              " 'extracted',\n",
              " 'situation',\n",
              " 'creative',\n",
              " 'livelybr',\n",
              " 'br',\n",
              " 'day',\n",
              " 'cartoon',\n",
              " 'history',\n",
              " 'friz',\n",
              " 'freleng',\n",
              " 'decided',\n",
              " 'pair',\n",
              " 'sylvester',\n",
              " 'departed',\n",
              " 'robert',\n",
              " 'clampetts',\n",
              " 'little',\n",
              " 'yellow',\n",
              " 'bird']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dVKxRsLseGo",
        "outputId": "da8bc003-b192-4aec-98b6-8d9a5ad06afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja21rUW_uFz5",
        "outputId": "d123e8bd-aadd-40f6-bdb4-34475720a2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gutenberg.sents('austen-sense.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']'], ['CHAPTER', '1'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6IAyB3teos6",
        "outputId": "f47eca97-181a-4a42-e4ac-ffbc1b28b0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "afinn = Afinn()\n",
        "(test[idx].decode('utf-8'))\n",
        "sentences = []\n",
        "for line in test:\n",
        "  sentences.append(line.decode('utf-8'))\n",
        "\n",
        "#sentences = test.decode('utf-8')\n",
        "print(sentences)\n",
        "#scored_sentences = ((afinn.score(sent), sent) for sent in sentences)\n",
        "#for sent in sentences:\n",
        "#  print(sent)\n",
        "scored_sentences\n",
        "sorted_sentences = sorted(scored_sentences)\n",
        "#print(\"\\n\".join(textwrap.wrap(sorted_sentences[0][1], 70)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-a313d936b3cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mafinn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAfinn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'decode'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZFeyfu1eNaA"
      },
      "source": [
        "sentences = (\" \".join(wordlist) for wordlist in gutenberg.sents('austen-sense.txt'))\n",
        "scored_sentences = ((afinn.score(sent), sent) for sent in sentences)\n",
        "scored_sentences\n",
        "sorted_sentences = sorted(scored_sentences)\n",
        "sorted_sentences\n",
        "#print(\"\\n\".join(textwrap.wrap(sorted_sentences[0][1], 70)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef5Zcbb91wEC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}